\documentclass{ctexart}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{siunitx}
\usepackage{amsthm}
\usepackage{geometry}
\geometry{a4paper, left=1in, right=1in, top=1in, bottom=1in}

\allowdisplaybreaks

\title{《人工智能与机器学习基础》第一次作业}
\author{PB24000150 李欣宸}
\date{\today}

\begin{document}
\maketitle

\section{第一题}

\subsection{(a)}

\newcommand{\et}{\mathbb{E}_{\mathrm{train}}}
\newcommand{\eyx}{\mathbb{E}_{y|\boldsymbol{x}}}
\newcommand{\fwt}{f_{\hat{w}(\mathrm{train})}}
\newcommand{\ft}{f_{\mathrm{true}}}
\newcommand{\fb}{\overline{f}}
\newcommand{\x}{\boldsymbol{x}}

\begin{proof}

考虑对一个固定的 $\x$，推导过程如下：

\begin{equation}
    \begin{aligned}
        & \et[\eyx[ (y - \fwt(\x))^2 ]] \\
        &= \et[\eyx[ ((y - \ft) + (\ft - \fwt))^2 ]] \\
        &= \underbrace{\eyx[ \epsilon^2 ]}_{\sigma^2~\text{的定义}} + \underbrace{2\et[\eyx[ \epsilon(\ft(\x) - \fwt(\x)) ]]}_{\epsilon~\text{和}~\ft(\x) - \fwt(\x)~\text{相互独立}} + \et[\eyx[ (\ft(\x) - \fwt(\x))^2 ]] \\
        &= \sigma^2 + 2\underbrace{\eyx[\epsilon]}_{=0}\et[ (\ft(\x) - \fwt(\x))] + \et[ (\ft(\x) - \fwt(\x))^2 ] \\
        &= \sigma^2 + \et[ (\ft(\x) - \fwt(\x))^2 ]
    \end{aligned}
    \label{1a1}
\end{equation}

而对于 $\et[ (\ft(\x) - \fwt(\x))^2 ]$，我们又有：

\begin{equation}
    \begin{aligned}
        & \et[ (\ft(\x) - \fwt(\x))^2 \\
        &= \et[ ((\ft(\x) - \fb(\x)) + (\fb(\x) - \fwt(\x)))^2 ] \\
        &= \underbrace{\et[ (\ft(\x) - \fb(\x))^2 ]}_{\mathrm{Bias}^2~\text{的定义}} + 2(\ft(\x) -\fb(\x))\underbrace{\et[ (\fb(\x) - \fwt(\x)) ]}_{\text{由}~\fb~\text{的定义，}=0} \\ &\quad+ \underbrace{\et[ (\fb(\x) - \fwt(\x))^2 ]}_{\mathrm{Var} 的定义} \\
        &= \mathrm{Bias}^2 + \mathrm{Var}
    \end{aligned}
    \label{1a2}
\end{equation}

综合 \eqref{1a1} 和 \eqref{1a2}，我们得到：

\begin{equation}
    \et[\eyx[ (y - \fwt(\x))^2 ]] = \sigma^2 + \mathrm{Bias}^2 + \mathrm{Var} \qedhere
\end{equation}

\end{proof}

\subsection{(b)}

由 $\mathrm{Bias}^2$ 的定义 $\mathrm{Bias}^2 = \et[(\ft(\x) - \fb(\x))^2]$，由于平方项的存在，$\mathrm{Bias}^2 \ge 0$ 恒成立。

当 $\ft \ne \fb$ 时，即模型的表示力不足以表达数据产生的规律时，$\mathrm{Bias}^2$ 永远大于 0，一个直观的例子就是，一条直线不能拟合不在同一直线上的 3 个点。

\subsection{(c)}

\begin{itemize}
    \item $\mathrm{Bias}^2$ 只与我们建立的数学模型能否表示数据真实产生的机制有关，与 $N$ 和 $n$ 并无直接关联；但是若采取题目中 $\fb$ 的定义，$\mathrm{Bias}^2$ 与 $n$ 的关系是：随着 $n$ 的增大逐渐向下趋于一个定值，这个定值与模型的表示力和 $N$ 的大小有关；
    \item $\mathrm{Var} = \mathcal O(\frac{1}{N})$
\end{itemize}

模型越复杂，$\mathrm{Bias}^2$ 越低，$\mathrm{Var}$ 越高；反之亦然。

\section{第二题}

\subsection{(a)}

\newcommand{\err}{\operatorname{err}}

\begin{proof}

根据 Hoeffding 不等式，对于独立有界随机变量 \(Z_i \in [a, b]\)，有：

\begin{equation}
    \Pr \left[ \left| \frac{1}{n} \sum_{i=1}^n Z_i - \mathbb{E}[Z_i] \right| > \epsilon \right] \leq 2 \exp \left( -\frac{2n\epsilon^2}{(b-a)^2} \right)
\label{hoeffding}
\end{equation}

取 $Z_i = I(h(x_i) \ne y_i)$，由定义，它是一个有界随机变量，$Z_i\in [0, 1]$。将 $a=0,b=1$ 带入 \eqref{hoeffding} 得：

\begin{equation}
    \Pr \left[ \left| \frac{1}{n} \sum_{i=1}^n Z_i - \mathbb{E}[Z_i] \right| > \epsilon \right] \leq 2 \exp (-2n\epsilon^2)
\end{equation}

而由 $Z_i$ 定义，经验误差 \(\err_S(h) = \frac{1}{n} \sum_{i=1}^n Z_i\)，期望误差 \(\err_D(h) = \mathbb{E}[Z_i]\)，则：

\begin{equation}
    \Pr[(\err_S(h) - \err_D(h)) > \epsilon] \le 2\exp(-2n\epsilon^2) \qedhere
\end{equation}

\end{proof}

\subsection{(b)}

\begin{proof}

\begin{equation}
    \begin{aligned}
        \Pr[\exists h \in H, (\err_S(h) - \err_D(h)) > \epsilon] &\le \sum_{h\in H} \Pr[(\err_S(h) - \err_D(h)) > \epsilon] \\
        &\le |H| \cdot 2 \exp(-2n\epsilon^2) \\
        &= 2|H|\exp(-2n\epsilon^2)
    \end{aligned}
    \label{22b}
    \qedhere
\end{equation}

\end{proof}

\subsection{(c)}

我们的数据偏差与训练集大小 $n$，模型类的大小 $|H|$ 有关：

\begin{itemize}
    \item 训练集大小 $n$：由 \eqref{22b}，其他条件不变时，$h$ 的泛化性不好的概率随 $n$ 的增长指数下降。这说明 $n$ 的大小对模型的泛化性非常显著；
    \item 模型类的大小 $|H|$：由于 $|H|$ 的大小随模型参数量的增加指数增长；由 \eqref{22b}，$h$ 泛化性不好的概率随模型类的大小线性增长，随模型参数量的增长指数增长。
\end{itemize}

这说明，训练参数量越大的模型，需要数据集大小也越多；如果认为 \eqref{22b} 给出的上界是一个紧的界，那么需要的数据集大小和参数量之间大概呈现出一个正比例关系。

为了减缓数据偏差带来的影响，我们要根据拥有数据集的大小选择合适的模型大小和正则化参数。

\section{第三题}

\subsection{(a)}

\begin{align}
\mathcal L(\mu^1, \mu^2, \Sigma) &= \prod_{i=1}^{n_1} p(x_i^1 | y_1) \prod_{i=1}^{n_2} p(x_i^2 | y_2) \nonumber \\
\implies \ln \mathcal L &= \sum_{i=1}^{n_1} \ln p(x_i^1 | y_1) + \sum_{i=1}^{n_2} \ln p(x_i^2 | y_2) \nonumber \\
&= -\frac{n_1 + n_2}{2} \ln 2\pi - \frac{n_1 + n_2}{2} \ln |\Sigma| \nonumber \\
&\quad -\frac{1}{2} \sum_{i=1}^{n_1} (x_i^1 - \mu^1)^T \Sigma^{-1} (x_i^1 - \mu^1) -\frac{1}{2} \sum_{i=1}^{n_2} (x_i^2 - \mu^2)^T \Sigma^{-1} (x_i^2 - \mu^2) \\
\text{由于} \arg &\max \mathcal L = \arg \max \ln \mathcal L, \text{ 我们要最大化 } \ln \mathcal L\text{，则：} \nonumber \\
\frac{\partial \ln \mathcal L}{\partial \mu^1} &= -\frac{1}{2} \sum_{i=1}^{n_1} \frac{\partial (x_i^1 - \mu^1)^T\Sigma^{-1} (x_i^1 - \mu^1)}{\partial \mu^1}  \nonumber \\
&= -\frac{1}{2}\sum_{i=1}^{n_1} -2 \Sigma^{-1} (x_i^1 - \mu^1) \nonumber \\
&= \sum_{i=1}^{n_1} \Sigma^{-1} (x_i^1 - \mu^1) = 0 \nonumber \\
\implies \quad \mu^1 &= \frac{1}{n_1} \sum_{i=1}^{n_1} x_i^1 \nonumber \\
\text{同理, } \mu^2 &= \frac{1}{n_2} \sum_{i=1}^{n_2} x_i^2 \quad \nonumber \\
\frac{\partial \ln \mathcal L}{\partial \Sigma} &= -\frac{n_1 + n_2}{2} \frac{\partial \ln |\Sigma|}{\partial \Sigma} - \frac{1}{2} \sum_{i=1}^{n_1} \frac{\partial (x_i^1 - \mu^1)^T \Sigma^{-1} (x_i^1 - \mu^1)}{\partial \Sigma} \nonumber \\
&= -\frac{1}{2} \sum_{i=1}^{n_2} \frac{\partial (x_i^2 - \mu^2)^T \Sigma^{-1} (x_i^2 - \mu^2)}{\partial \Sigma} \nonumber \\
&= -\frac{n_1 + n_2}{2} \Sigma^{-1} + \frac{1}{2} \sum_{i=1}^{n_1} \Sigma^{-1} (x_i^1 - \mu^1) (x_i^1 - \mu^1)^T \Sigma^{-1} \nonumber \\
&\quad + \frac{1}{2} \sum_{i=1}^{n_2} \Sigma^{-1} (x_i^2 - \mu^2) (x_i^2 - \mu^2)^T \Sigma^{-1} \nonumber \\
&= \frac{1}{2} \Sigma^{-1} \left( -(n_1 + n_2) \Sigma + \sum_{i=1}^{n_1} (x_i^1 - \mu^1) (x_i^1 - \mu^1)^T + \sum_{i=1}^{n_2} (x_i^2 - \mu^2) (x_i^2 - \mu^2)^T \right) \Sigma^{-1} = 0 \nonumber \\
\implies \quad \Sigma &= \frac{1}{n_1 + n_2} \left( \sum_{i=1}^{n_1} (x_i^1 - \mu^1) (x_i^1 - \mu^1)^T + \sum_{i=1}^{n_2} (x_i^2 - \mu^2) (x_i^2 - \mu^2)^T \right) \nonumber \\
\text{综上，显式}&\text{解为：} \nonumber \\
\mu^1 &= \frac{1}{n_1} \sum_{i=1}^{n_1} x_i^1, \quad \mu^2 = \frac{1}{n_2} \sum_{i=1}^{n_2} x_i^2,  \label{mu} \\
\Sigma &= \frac{1}{n_1 + n_2} \left( \sum_{i=1}^{n_1} (x_i^1 - \mu^1) (x_i^1 - \mu^1)^T + \sum_{i=1}^{n_2} (x_i^2 - \mu^2) (x_i^2 - \mu^2)^T \label{Sigma} \right)
\end{align}

\subsection{(b)}

考虑将所有的数据堆叠成批，可以得到输入矩阵：

\begin{gather}
X^1 = \begin{pmatrix}
2.0 & 2.5 & 3.0 & 2.2 & 2.8 \\
2.5 & 2.8 & 2.7 & 3.0 & 2.6 \\
2.0 & 2.2 & 2.5 & 2.3 & 2.4
\end{pmatrix}, \quad
X^2 = 
\begin{pmatrix}
3.5 & 3.2 & 3.8 & 3.0 & 4.0 \\
3.8 & 4.0 & 3.5 & 3.9 & 3.6 \\
3.2 & 3.5 & 3.7 & 3.3 & 3.9
\end{pmatrix} 
\end{gather}

带入 \eqref{mu},\eqref{Sigma} 可得：

\begin{gather}
\mu^1 = \begin{pmatrix}
2.5 \\
2.72 \\
2.28
\end{pmatrix}, \quad
\mu^2 =
\begin{pmatrix}
3.5 \\
3.76 \\
3.52
\end{pmatrix}, \quad
\Sigma = \begin{pmatrix}
0.136  & -0.032  &  0.064 \\
-0.032 &  0.032  & -0.0114 \\
0.064  & -0.0114 &  0.0476
\end{pmatrix}
\end{gather}

由于 $x|y_i \sim \mathcal N(\mu^i, \Sigma),~i=1,2$，则有：

\begin{equation}
\begin{aligned}
p(x|y_1) &= \frac{1}{(2\pi)^{3/2} |\Sigma|^{1/2}} \exp \left\{ -\frac{1}{2} (x - \mu^1)^T \Sigma^{-1} (x - \mu^1) \right\} \\
p(x|y_2) &= \frac{1}{(2\pi)^{3/2} |\Sigma|^{1/2}} \exp \left\{ -\frac{1}{2} (x - \mu^2)^T \Sigma^{-1} (x - \mu^2) \right\}
\end{aligned}
\end{equation}

代入 $x = \begin{pmatrix}2.7&2.9&3.5\end{pmatrix}^T$，可得 $p(x|y_1) = \num{1.3115e-15},~p(x|y_2) = \num{5.9952e-14}$，由于输入数据中两个分类类别各半，取 $p(y_1) = p(y_2) = 0.5$，则由 Bayes 公式可得：

\begin{align}
p(y_1|x) &= \frac{p(x|y_1)p(y_1)}{p(x|y_1)p(y_1) + p(x|y_2)p(y_2)} \approx 0.0214 = \SI{2.14}{\percent} \\
p(y_2|x) &= \frac{p(x|y_2)p(y_2)}{p(x|y_1)p(y_1) + p(x|y_2)p(y_2)} \approx 0.9786 = \SI{97.86}{\percent} \label{py2x}
\end{align}

由 \eqref{py2x} 可知，该样本更有可能属于类别 $y_2$（即标签 1），概率为约 \SI{97.86}{\percent}。

\subsection{(c)}

\begin{equation}
\begin{aligned}
z &= \ln \frac{p(x|y_1)p(y_1)}{p(x|y_2)p(y_2)} \\
&= \ln \frac{n_1}{n_2} + \ln \frac{\exp \left\{ -\frac{1}{2} (x - \mu^1)^T \Sigma^{-1} (x - \mu^1) \right\}}{\exp \left\{ -\frac{1}{2} (x - \mu^2)^T \Sigma^{-1} (x - \mu^2) \right\}} \\
&= \ln \frac{n_1}{n_2} - \frac{1}{2} \left( (x - \mu^1)^T \Sigma^{-1} (x - \mu^1) - (x - \mu^2)^T \Sigma^{-1} (x - \mu^2) \right) \\
&= \ln \frac{n_1}{n_2} - \frac{1}{2} \left( x^T \Sigma^{-1} x - 2 (\mu^1)^T \Sigma^{-1} x + (\mu^1)^T \Sigma^{-1} \mu^1 - x^T \Sigma^{-1} x + 2 (\mu^2)^T \Sigma^{-1} x - (\mu^2)^T \Sigma^{-1} \mu^2 \right) \\
&= \left( (\mu^1 - \mu^2)^T \Sigma^{-1} \right) x + \frac{1}{2} \left( (\mu^2)^T \Sigma^{-1} \mu^2 - (\mu^1)^T \Sigma^{-1} \mu^1 \right) + \ln \frac{n_1}{n_2} \label{z}
\end{aligned}
\end{equation}

观察到 \eqref{z} 符合 $z = w \cdot x + b$ 的形式，其中：

\begin{equation}
\begin{aligned}
w &= (\mu^1 - \mu^2)^T \Sigma^{-1} \\
b &= \frac{1}{2} \left( (\mu^2)^T \Sigma^{-1} \mu^2 - (\mu^1)^T \Sigma^{-1} \mu^1 \right) + \ln \frac{n_1}{n_2}
\end{aligned}
\end{equation}

\section{第四题}

\subsection{(a)}

\begin{gather}
x = \begin{pmatrix} 3 \\ 14 \end{pmatrix} \\
a^1 = f^1(z) = \max\{0, (W^1)^T x + w^1_0\}  = \begin{pmatrix} 2 & 13 & 0 & 0 \end{pmatrix}^T \\
a^2 = f^2(z) = \mathsf{Softmax}((W^2)^T a^1 + w^2_0) = \mathsf{Softmax}(\begin{pmatrix} 15 \\ -13\end{pmatrix}) = \begin{pmatrix}1-e^{-28}\\e^{-28}\end{pmatrix} \approx \begin{pmatrix} 1-\num{6.91e-13} \\ \num{6.91e-13} \end{pmatrix}
\end{gather}

\subsection{(b)}

\begin{gather}
X = \begin{pmatrix} 0.5 & 0 & -3 \\ 0.5 & 2 & 0.5 \end{pmatrix} \\
f^1(Z^1) = \max\{0, (W^1)^T X + w^1_0\} = \begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0 \end{pmatrix} \\ 
\end{gather}

\subsection{(c)}

先求梯度：

\begin{gather}
\frac{\partial \mathcal L}{\partial z^2} = \mathsf{Softmax}(z^2) - y = \begin{pmatrix} 1 - e^{-28} \\ e^{-28} - 1 \end{pmatrix} \approx \begin{pmatrix} 1 \\ -1 \end{pmatrix} \\
\frac{\partial z^2}{\partial W^2} = a^1 
\implies \frac{\partial \mathcal L}{\partial W^2} = a^1 \cdot \left( \mathsf{Softmax}(z^2) - y \right)^T = \begin{pmatrix} 2 & -2 \\ 13 & -13 \\ 0 & 0 \\ 0 & 0 \end{pmatrix} \\
\frac{\partial z^2}{\partial w^2_0} = 1
\implies \frac{\partial \mathcal L}{\partial w^2_0} = \frac{\partial \mathcal L}{\partial z^2} \cdot \frac{\partial z^2}{\partial w^2_0} = \begin{pmatrix} 1 \\ -1 \end{pmatrix} \\
\frac{\partial z^2}{\partial a^1} = W^2
\implies \frac{\partial \mathcal L}{\partial a^1} = \frac{\partial z^2}{\partial a^1} \cdot \frac{\partial \mathcal L}{\partial z^2} = \begin{pmatrix}1 & -1\\1 & -1 \\ 1 & -1 \\ 1 & -1\end{pmatrix} \begin{pmatrix} 1\\-1\end{pmatrix}=\begin{pmatrix}2 \\ 2 \\ 2 \\ 2\end{pmatrix} \\
\frac{\partial a^1}{\partial z^1} = \operatorname{diag}(I_{z^1 > 0}) \implies \frac{\partial \mathcal L}{\partial z^1} = \frac{\partial a^1}{\partial z^1} \cdot \frac{\partial \mathcal L}{\partial a^1} = \begin{pmatrix} 2 \\ 2 \\ 0 \\ 0 \end{pmatrix} \\
\frac{\partial z^1}{\partial W^1} = x \implies \frac{\partial \mathcal L}{\partial W^1} = x \cdot \left( \frac{\partial \mathcal L}{\partial z^1} \right)^T = \begin{pmatrix} 6 & 6 & 0 & 0 \\ 28 & 28 & 0 & 0 \end{pmatrix} \\
\frac{\partial z^1}{\partial w^1_0} = 1 \implies \frac{\partial \mathcal L}{\partial w^1_0} = \frac{\partial \mathcal L}{\partial z^1} \cdot \frac{\partial z^1}{\partial w^1_0} = \begin{pmatrix} 2 \\ 2 \\ 0 \\ 0 \end{pmatrix}   
\end{gather}

再根据学习率 $\eta = 0.1$ 更新参数：

\begin{gather}
W^1 \gets W^1 - \eta \frac{\partial \mathcal L}{\partial W^1} = \begin{pmatrix} 1 & 0 & -1 & 0 \\ 0 & 1 & 0 & -1\end{pmatrix} - 0.1 \begin{pmatrix} 6 & 6 & 0 & 0 \\ 28 & 28 & 0 & 0 \end{pmatrix} = \begin{pmatrix} 0.4 & -0.6 & -0.1 & 0 \\ -2.8 & -1.8 & 0 & -0.1 \end{pmatrix} \\
w^1_0 \gets w^1_0 - \eta \frac{\partial \mathcal L}{\partial w^1_0} = \begin{pmatrix} -1 \\ -1 \\ -1 \\ -1 \end{pmatrix} - 0.1 \begin{pmatrix} 2 \\ 2 \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} -1.2 \\ -1.2 \\ -1 \\ -1 \end{pmatrix} \\
W^2 \gets W^2 - \eta \frac{\partial \mathcal L}{\partial W^2} = \begin{pmatrix} 1 & -1 \\ 1 & -1 \\ 1 & -1 \\ 1 & -1 \end{pmatrix} - 0.1 \begin{pmatrix} 2 & -2 \\ 13 & -13 \\ 0 & 0 \\ 0 & 0 \end{pmatrix} = \begin{pmatrix} 0.8 & -0.8 \\ -0.3 & 0.3 \\ 1 & -1 \\ 1 & -1 \end{pmatrix} \\
w^2_0 \gets w^2_0 - \eta \frac{\partial \mathcal L}{\partial w^2_0} = \begin{pmatrix}0 \\ 2 \end{pmatrix} - 0.1 \begin{pmatrix} 1 \\ -1 \end{pmatrix} = \begin{pmatrix} -0.1 \\ 2.1 \end{pmatrix}
\end{gather}
\end{document}