{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5202fc3f",
   "metadata": {},
   "source": [
    "# 决策树分类实验（Decision Tree Classification）\n",
    "本实验将带你从零理解并使用决策树进行分类任务，包括熵、信息增益、模型训练、可视化和参数调节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b7155a",
   "metadata": {},
   "source": [
    "## 1. 实验介绍\n",
    "在本实验中，你将：\n",
    "- 理解熵（Entropy）与信息增益（Information Gain）的含义\n",
    "- 使用 sklearn 训练决策树\n",
    "- 可视化决策边界与树结构\n",
    "- 比较不同参数对模型的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cc3050",
   "metadata": {},
   "source": [
    "## 2. 实验大纲\n",
    "- [ 1 - Packages ](#1)\n",
    "- [ 2 -  Problem Statement](#2)\n",
    "- [ 3 - Dataset](#3)\n",
    "  - [ 3.1 One hot encoded dataset](#3.1)\n",
    "- [ 4 - Decision Tree Refresher](#4)\n",
    "  - [ 4.1  Calculate entropy](#4.1)\n",
    "    - [ Exercise 1](#ex01)\n",
    "  - [ 4.2  Split dataset](#4.2)\n",
    "    - [ Exercise 2](#ex02)\n",
    "  - [ 4.3  Calculate information gain](#4.3)\n",
    "    - [ Exercise 3](#ex03)\n",
    "  - [ 4.4  Get best split](#4.4)\n",
    "    - [ Exercise 4](#ex04)\n",
    "- [ 5 - Building the tree](#5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c18fdc",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## 1 - 导入相关库（Packages）\n",
    "\n",
    "首先，让我们运行下面的代码单元来导入本次实验所需的全部库。\n",
    "\n",
    "- [numpy](https://www.numpy.org) 是 Python 中用于处理矩阵和多维数组的基础科学计算包。\n",
    "- [matplotlib](https://matplotlib.org) 是一个用于绘制图形的著名 Python 可视化库。\n",
    "- `utils.py` 文件中包含了本次作业会使用到的一些辅助函数。你**不需要修改**其中的任何代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8eb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879deff7",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "## 2 - 问题描述（Problem Statement）\n",
    "\n",
    "假设你正在创办一家种植并销售野生蘑菇的公司。  \n",
    "- 由于并不是所有蘑菇都可食用，你希望能够根据蘑菇的外部物理属性来判断某个蘑菇是否可食用或有毒。\n",
    "- 你已经收集了一些现有的数据，可以用于完成这一任务。\n",
    "\n",
    "你能否利用这些数据来帮助你识别哪些蘑菇可以安全出售？\n",
    "\n",
    "注意：本实验使用的数据集仅用于 **教学和示例目的**，不应被视为现实中识别可食用蘑菇的指导依据。\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"3\"></a>\n",
    "## 3 - 数据集（Dataset）\n",
    "\n",
    "你将从加载本任务所需的数据集开始。你已经收集到的数据如下：\n",
    "\n",
    "| 帽子颜色（Cap Color） | 茎部形状（Stalk Shape） | 是否独生（Solitary） | 可食用（Edible） |\n",
    "|:---------------------:|:-----------------------:|:--------------------:|:----------------:|\n",
    "|        Brown         |        Tapering         |         Yes          |        1         |\n",
    "|        Brown         |        Enlarging        |         Yes          |        1         |\n",
    "|        Brown         |        Enlarging        |         No           |        0         |\n",
    "|        Brown         |        Enlarging        |         No           |        0         |\n",
    "|        Brown         |        Tapering         |         Yes          |        1         |\n",
    "|         Red          |        Tapering         |         Yes          |        0         |\n",
    "|         Red          |        Enlarging        |         No           |        0         |\n",
    "|        Brown         |        Enlarging        |         Yes          |        1         |\n",
    "|         Red          |        Tapering         |         No           |        1         |\n",
    "|        Brown         |        Enlarging        |         No           |        0         |\n",
    "\n",
    "你一共有 **10 个蘑菇样本（examples）**，每个样本包含：\n",
    "\n",
    "- 三个特征（features）  \n",
    "  - 帽子颜色 Cap Color（`Brown` 或 `Red`）  \n",
    "  - 茎部形状 Stalk Shape（`Tapering` 或 `Enlarging`）  \n",
    "  - 是否独生 Solitary（`Yes` 或 `No`）  \n",
    "- 一个标签（label）  \n",
    "  - Edible（可食用，`1` 表示可食用，`0` 表示有毒）\n",
    "\n",
    "---\n",
    "\n",
    "<a name=\"3.1\"></a>\n",
    "### 3.1 One-hot 编码后的数据集（One hot encoded dataset）\n",
    "\n",
    "为了便于后续算法实现，我们对特征进行了 **One-hot 编码**（即将分类特征转成 0 或 1 的二值特征）。\n",
    "\n",
    "| Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:---------:|:--------------------:|:--------:|:------:|\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "|     1     |           1          |     1    |    1   |\n",
    "|     0     |           1          |     1    |    0   |\n",
    "|     0     |           0          |     0    |    0   |\n",
    "|     1     |           0          |     1    |    1   |\n",
    "|     0     |           1          |     0    |    1   |\n",
    "|     1     |           0          |     0    |    0   |\n",
    "\n",
    "因此：\n",
    "\n",
    "- `X_train` 中每个样本包含三个特征：  \n",
    "  - **Brown Color**：`1` 表示帽子颜色为 Brown，`0` 表示 Red  \n",
    "  - **Tapering Shape**：`1` 表示茎部形状为 Tapering，`0` 表示 Enlarging  \n",
    "  - **Solitary**：`1` 表示 Yes，`0` 表示 No  \n",
    "\n",
    "- `y_train` 表示蘑菇是否可食用  \n",
    "  - `y = 1` → 可食用  \n",
    "  - `y = 0` → 有毒  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19089f50",
   "metadata": {},
   "source": [
    "#### 查看变量（View the variables）\n",
    "\n",
    "让我们更熟悉一下你的数据集。  \n",
    "- 一个很好的开始方式就是直接打印出每个变量，并查看它们包含的内容。\n",
    "\n",
    "下面的代码将打印 `X_train` 的前几个元素，以及该变量的类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a31390",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few elements of X_train:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3514119",
   "metadata": {},
   "source": [
    "对y_train也做同样的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da0221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First few elements of y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c2d41",
   "metadata": {},
   "source": [
    "#### 检查变量的维度（Check the dimensions of your variables）\n",
    "\n",
    "另一种熟悉数据集的有用方式是查看它们的维度（dimensions）。\n",
    "\n",
    "请打印 `X_train` 和 `y_train` 的形状（shape），并观察你的数据集中有多少个训练样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d6ca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276e993",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "## 4 - 决策树复习（Decision Tree Refresher）\n",
    "\n",
    "在本实验练习中，你将基于提供的数据集构建一棵决策树。\n",
    "\n",
    "- 回顾一下，构建决策树的步骤如下：\n",
    "    - 在根节点（root node）开始，使用全部样本\n",
    "    - 计算在所有可能特征上进行划分（split）的信息增益（information gain），并选择信息增益最大的那个特征\n",
    "    - 根据所选特征对数据集进行划分，并在树中创建左、右分支\n",
    "    - 持续重复划分过程，直到满足停止条件（stopping criteria）\n",
    "  \n",
    "- 在本次实验中，你将实现以下函数，这些函数将允许你基于信息增益最高的特征，将某个节点划分成左、右子分支：\n",
    "    - 计算某个节点的熵（entropy）\n",
    "    - 使用给定特征将节点处的数据集划分为左、右子集\n",
    "    - 计算基于某特征进行划分所获得的信息增益\n",
    "    - 选择使信息增益最大的特征\n",
    "    \n",
    "- 然后，我们将使用你实现的这些辅助函数，通过不断重复划分过程，构建一棵决策树，直到达到停止条件为止。\n",
    "    - 在本实验中，我们选择的停止条件是：**将最大深度（max depth）设为 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184e817a",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "### 4.1  计算熵（Calculate entropy）\n",
    "\n",
    "首先，你将编写一个名为 `compute_entropy` 的辅助函数，用来计算某个节点的熵（entropy，也是不纯度 impurity 的度量）。\n",
    "\n",
    "- 该函数接收一个 numpy 数组（`y`），其中每个元素表示该节点中的样本是否可食用：  \n",
    "  - `1` 表示可食用（edible）  \n",
    "  - `0` 表示有毒（poisonous）\n",
    "\n",
    "请完成下面的 `compute_entropy()` 函数，使其能够：\n",
    "\n",
    "* 计算 \\(p_1\\)，即该节点中可食用样本的比例  \n",
    "  （也就是 `y` 中值为 `1` 的样本占比）\n",
    "\n",
    "* 熵的计算公式如下：\n",
    "\n",
    "$$\n",
    "H(p_1) = -p_1 \\log_2(p_1) - (1 - p_1)\\log_2(1 - p_1)\n",
    "$$\n",
    "\n",
    "* 注意事项  \n",
    "    * 对数的底数为 \\(2\\)  \n",
    "    * 在实际实现中，规定 $0 / log_2(0) = 0$。也就是说，如果 `p_1 = 0` 或 `p_1 = 1`，则熵应设为 `0`\n",
    "    * 请确保节点中的数据非空（即 `len(y) != 0`）。如果为空，则返回 `0`\n",
    "\n",
    "<a name=\"ex01\"></a>\n",
    "### 练习 1（Exercise 1）\n",
    "\n",
    "请根据前面的说明完成 `compute_entropy()` 函数的实现。\n",
    "\n",
    "如果你在实现过程中遇到困难，可以查看本单元后给出的提示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ba2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: compute_entropy\n",
    "\n",
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for \n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           edible (`1`) or poisonous (`0`)\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "        \n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly\n",
    "    entropy = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "           \n",
    "    ### END CODE HERE ###        \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f500de7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>点击查看提示（Click for hints）</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * 计算 `p1` 的提示\n",
    "       * 你可以通过 `y[y == 1]` 获取 `y` 中值为 `1` 的样本子集\n",
    "       * 可以用 `len(y)` 获取 `y` 中样本总数\n",
    "   * 计算 `entropy` 的提示\n",
    "       * <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log2.html\">np.log2</a> 可以用来对 numpy 数组计算以 2 为底的对数\n",
    "       * 如果 `p1` 等于 0 或 1，请记得将熵设为 `0`（处理 0 log 0 的情况）\n",
    "\n",
    "\n",
    "</details>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677470c8-383d-4309-9e3a-7e1702e495c8",
   "metadata": {},
   "source": [
    "<details>\n",
    "        <summary><font size=\"2\" color=\"darkblue\"><b> 点击查看更多提示（Click for more hints）</b></font></summary>\n",
    "        \n",
    "    * 下面是你可以参考的函数实现结构示例：\n",
    "    \n",
    "```python\n",
    "    def compute_entropy(y):\n",
    "        \n",
    "        # 你需要正确返回以下变量\n",
    "        entropy = 0.\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        if len(y) != 0:\n",
    "            # 你的代码：计算可食用样本（值为 1）的比例 p1\n",
    "            p1 =\n",
    "\n",
    "            # 当 p1 = 0 或 1 时，熵应该为 0（处理 0log0）\n",
    "            if p1 != 0 and p1 != 1:\n",
    "                # 你的代码：根据上面的公式计算熵\n",
    "                entropy = \n",
    "            else:\n",
    "                entropy = 0.\n",
    "        ### END CODE HERE ###        \n",
    "\n",
    "        return entropy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382440e",
   "metadata": {},
   "source": [
    "你可以运行下面的测试代码来检查你的实现是否正确："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute entropy at the root node (i.e. with all examples)\n",
    "# Since we have 5 edible and 5 non-edible mushrooms, the entropy should be 1\"\n",
    "\n",
    "print(\"Entropy at root node: \", compute_entropy(y_train)) \n",
    "\n",
    "# UNIT TESTS\n",
    "compute_entropy_test(compute_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39323df",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "### 4.2  划分数据集（Split dataset）\n",
    "\n",
    "接下来，你将编写一个名为 `split_dataset` 的辅助函数。  \n",
    "该函数接收某个节点处的数据以及要用于划分的特征，并将数据划分为左分支与右分支。  \n",
    "在实验的后续部分，你将实现代码来计算这个划分的质量（即信息增益）。\n",
    "\n",
    "- 该函数接收训练数据、该节点中样本的索引列表（`node_indices`），以及用于划分的特征编号。\n",
    "- 它根据该特征对数据进行划分，并返回左分支和右分支对应的索引子集。\n",
    "- 例如，假设我们当前位于根节点（因此 `node_indices = [0,1,2,3,4,5,6,7,8,9]`），并选择使用特征 `0`（表示样本是否具有棕色菌盖）进行划分：\n",
    "    - 则函数输出为：  \n",
    "      `left_indices = [0,1,2,3,4,7,9]`  \n",
    "      `right_indices = [5,6,8]`\n",
    "\n",
    "| 索引（Index） | Brown Cap | Tapering Stalk Shape | Solitary | Edible |\n",
    "|:-------------:|:---------:|:--------------------:|:--------:|:------:|\n",
    "|       0       |     1     |           1          |     1    |    1   |\n",
    "|       1       |     1     |           0          |     1    |    1   |\n",
    "|       2       |     1     |           0          |     0    |    0   |\n",
    "|       3       |     1     |           0          |     0    |    0   |\n",
    "|       4       |     1     |           1          |     1    |    1   |\n",
    "|       5       |     0     |           1          |     1    |    0   |\n",
    "|       6       |     0     |           0          |     0    |    0   |\n",
    "|       7       |     1     |           0          |     1    |    1   |\n",
    "|       8       |     0     |           1          |     0    |    1   |\n",
    "|       9       |     1     |           0          |     0    |    0   |\n",
    "\n",
    "<a name=\"ex02\"></a>\n",
    "### 练习 2（Exercise 2）\n",
    "\n",
    "请完成下面提供的 `split_dataset()` 函数。\n",
    "\n",
    "- 对于 `node_indices` 中的每一个索引：\n",
    "    - 如果该样本在所选特征上的值为 `1`，则将该索引加入 `left_indices`\n",
    "    - 如果值为 `0`，则将该索引加入 `right_indices`\n",
    "\n",
    "如果你在实现过程中卡住，可以查看该代码单元之后提供的提示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c627a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: split_dataset\n",
    "\n",
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (ndarray):  List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (ndarray): Indices with feature value == 1\n",
    "        right_indices (ndarray): Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "           \n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee19936",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Here's how you can structure the overall implementation for this function\n",
    "\n",
    "```python \n",
    "    def split_dataset(X, node_indices, feature):\n",
    "    \n",
    "        ### You need to return the following variables correctly\n",
    "        left_indices = []\n",
    "        right_indices = []\n",
    "\n",
    "        #### START CODE HERE ###\n",
    "     \n",
    "        ### Go through the indices of examples at that node\n",
    "        for i in node_indices:   \n",
    "            if # Your code here to check if the value of X at that index for the feature is 1\n",
    "                left_indices.append(i)\n",
    "            else:\n",
    "                right_indices.append(i)\n",
    "        #### END CODE HERE ###\n",
    "        \n",
    "        return left_indices, right_indices\n",
    "```\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><font size=\"2\" color=\"darkblue\"><b> Click for more hints</b></font></summary>\n",
    "    The condition is <code>if X[i][feature] == 1:</code>.\n",
    "        \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d785c2",
   "metadata": {},
   "source": [
    "现在，让我们使用下面的代码块来检查你的实现是否正确。  \n",
    "我们将尝试在根节点（其中包含所有样本）使用特征 0（Brown Cap）对数据集进行划分，就像我们之前讨论的那样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8537f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Feel free to play around with these variables\n",
    "# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "# UNIT TESTS    \n",
    "split_dataset_test(split_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c32bb6",
   "metadata": {},
   "source": [
    "<a name=\"4.3\"></a>\n",
    "### 4.3  计算信息增益（Calculate information gain）\n",
    "\n",
    "接下来，你将编写一个名为 `information_gain` 的函数。  \n",
    "该函数接收训练数据、某节点中的样本索引，以及用于划分的特征编号，并返回基于该划分所获得的信息增益。\n",
    "\n",
    "<a name=\"ex03\"></a>\n",
    "### 练习 3（Exercise 3）\n",
    "\n",
    "请完成下面的 `compute_information_gain()` 函数，以计算：\n",
    "\n",
    "$$\n",
    "\\text{Information Gain} = H(p_1^{\\text{node}}) - \\big( \n",
    "w^{\\text{left}} H(p_1^{\\text{left}}) \\;+\\; \n",
    "w^{\\text{right}} H(p_1^{\\text{right}})\n",
    "\\big)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $H(p_1^{\\text{node}})$：当前节点的数据熵  \n",
    "- $H(p_1^{\\text{left}})$ 与 $H(p_1^{\\text{right}})$：基于划分后得到的左、右子节点的熵  \n",
    "- $w^{\\text{left}}$ 与 $w^{\\text{right}}$：左、右子节点中样本数占该节点样本总数的比例  \n",
    "\n",
    "注意：\n",
    "\n",
    "- 你可以直接使用之前实现的 `compute_entropy()` 函数来计算熵  \n",
    "- 我们已经提供了部分起始代码，其中使用了你在上一部分实现的 `split_dataset()` 函数来对数据集进行划分\n",
    "\n",
    "如果你在实现过程中遇到困难，可以查看该代码单元后给出的提示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669b6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: compute_information_gain\n",
    "\n",
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "   \n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    information_gain = 0\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Weights \n",
    "    \n",
    "    #Weighted entropy\n",
    "     \n",
    "    #Information gain                                                   \n",
    "    \n",
    "    ### END CODE HERE ###  \n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2540a4",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Here's how you can structure the overall implementation for this function\n",
    "```python \n",
    "    def compute_information_gain(X, y, node_indices, feature):\n",
    "        # Split dataset\n",
    "        left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "\n",
    "        # Some useful variables\n",
    "        X_node, y_node = X[node_indices], y[node_indices]\n",
    "        X_left, y_left = X[left_indices], y[left_indices]\n",
    "        X_right, y_right = X[right_indices], y[right_indices]\n",
    "\n",
    "        # You need to return the following variables correctly\n",
    "        information_gain = 0\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Your code here to compute the entropy at the node using compute_entropy()\n",
    "        node_entropy = \n",
    "        # Your code here to compute the entropy at the left branch\n",
    "        left_entropy = \n",
    "        # Your code here to compute the entropy at the right branch\n",
    "        right_entropy = \n",
    "\n",
    "        # Your code here to compute the proportion of examples at the left branch\n",
    "        w_left = \n",
    "        \n",
    "        # Your code here to compute the proportion of examples at the right branch\n",
    "        w_right = \n",
    "\n",
    "        # Your code here to compute weighted entropy from the split using \n",
    "        # w_left, w_right, left_entropy and right_entropy\n",
    "        weighted_entropy = \n",
    "\n",
    "        # Your code here to compute the information gain as the entropy at the node\n",
    "        # minus the weighted entropy\n",
    "        information_gain = \n",
    "        ### END CODE HERE ###  \n",
    "\n",
    "        return information_gain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7bf476",
   "metadata": {},
   "source": [
    "现在，你可以使用下面的代码单元来检查你的实现，并计算在每个特征上进行划分时所得到的信息增益。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58c2da-8913-434f-953f-da2ef14364de",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "    \n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)\n",
    "\n",
    "# UNIT TESTS\n",
    "compute_information_gain_test(compute_information_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a709528",
   "metadata": {},
   "source": [
    "在根节点对特征 “Solitary”（特征编号 = 2）进行划分，会获得最大的的信息增益。因此，它是根节点上最优的划分特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e683577",
   "metadata": {},
   "source": [
    "<a name=\"4.4\"></a>\n",
    "### 4.4  获取最佳划分特征（Get best split）\n",
    "\n",
    "现在，让我们编写一个函数，通过计算每个特征的信息增益（就像我们之前所做的那样），并返回能够产生最大信息增益的那个特征，即最佳划分特征。\n",
    "\n",
    "<a name=\"ex04\"></a>\n",
    "### 练习 4（Exercise 4）\n",
    "\n",
    "请完成下面给出的 `get_best_split()` 函数。\n",
    "\n",
    "- 该函数接收训练数据以及该节点中所有数据点的索引。\n",
    "- 函数的输出是：能够产生最大信息增益的特征编号  \n",
    "    - 你可以使用 `compute_information_gain()` 函数，对每个特征进行遍历，并计算该特征对应的信息增益\n",
    "\n",
    "如果在实现过程中遇到困难，你可以查看该代码单元之后提供的提示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7aaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C4\n",
    "# GRADED FUNCTION: get_best_split\n",
    "\n",
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "       \n",
    "    ### END CODE HERE ##    \n",
    "   \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3f6473",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><font size=\"3\" color=\"darkgreen\"><b>Click for hints</b></font></summary>\n",
    "    \n",
    "    \n",
    "   * Here's how you can structure the overall implementation for this function\n",
    "    \n",
    "```python \n",
    "    def get_best_split(X, y, node_indices):   \n",
    "\n",
    "        # Some useful variables\n",
    "        num_features = X.shape[1]\n",
    "\n",
    "        # You need to return the following variables correctly\n",
    "        best_feature = -1\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        max_info_gain = 0\n",
    "\n",
    "        # Iterate through all features\n",
    "        for feature in range(num_features): \n",
    "            \n",
    "            # Your code here to compute the information gain from splitting on this feature\n",
    "            info_gain = \n",
    "            \n",
    "            # If the information gain is larger than the max seen so far\n",
    "            if info_gain > max_info_gain:  \n",
    "                # Your code here to set the max_info_gain and best_feature\n",
    "                max_info_gain = \n",
    "                best_feature = \n",
    "        ### END CODE HERE ##    \n",
    "   \n",
    "    return best_feature\n",
    "```\n",
    "If you're still stuck, check out the hints below.\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b> Hint to calculate info_gain</b></font></summary>\n",
    "        \n",
    "<code>info_gain = compute_information_gain(X, y, node_indices, feature)</code>\n",
    "</details>\n",
    "    \n",
    "<details>\n",
    "          <summary><font size=\"2\" color=\"darkblue\"><b>Hint to update the max_info_gain and best_feature</b></font></summary>\n",
    "           <code>max_info_gain = info_gain</code><br>\n",
    "           <code>best_feature = feature</code>\n",
    "</details>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a5027",
   "metadata": {},
   "source": [
    "现在，让我们使用下面的代码单元来检查你所编写函数的实现是否正确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62648e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)\n",
    "\n",
    "# UNIT TESTS\n",
    "get_best_split_test(get_best_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b8c681",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - 构建决策树（Building the tree）\n",
    "\n",
    "在本节中，我们将使用你在前面实现的那些函数，通过不断选择最佳特征进行划分，来逐步生成一棵决策树，直到达到停止条件（本实验中，最大深度设为 2）。\n",
    "\n",
    "在这一部分中，你不需要实现任何新代码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7837608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not graded\n",
    "tree = []\n",
    "\n",
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    tree.append((current_depth, branch_name, best_feature, node_indices))\n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai25)",
   "language": "python",
   "name": "ai25"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
